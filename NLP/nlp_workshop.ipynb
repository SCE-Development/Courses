{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCE NLP Workshop\n",
    "\n",
    "Hey there! Thanks for checking out my workshop. This notebook has code snippets to help you implement your own solution to the [NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) contest on Kaggle.\n",
    "\n",
    "[Click here]() to open this notebook in Google Colab (free GPUs!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np        # linear algebra\n",
    "import pandas as pd       # data processing\n",
    "from tqdm import tqdm     # progress bars\n",
    "import matplotlib.pyplot as plt  # plot graphs\n",
    "\n",
    "\n",
    "np.random.RandomState(123) # seed RNG for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data exploration\n",
    "\n",
    "Load the training data and checkout what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df_ori = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "\n",
    "df_ori # data frame original: contains unmodified training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at samples from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 10 keywords\n",
    "'''TODO'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 10 locations\n",
    "'''TODO'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 10 posts\n",
    "for index, text in df_ori['text'].iteritems():\n",
    "    # repr() will print special characters as escaped\n",
    "    # e.g. '\\n' for newline\n",
    "    \n",
    "    '''TODO'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to ignore 'id' and 'location' because they're useless. 'keyword' might be helpful, but we'll ignore that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns (axis=1)\n",
    "df = '''TODO: drop the id and location columns'''\n",
    "\n",
    "df.shape # returns the dimensions of the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a transformer, which maps a sentence into a sentence embedding (i think?). You can think of it as transforming sentences into vectors that represent the sentence.\n",
    "\n",
    "We're going to use a pretrained model from [Hugging Face](https://huggingface.co/transformers/model_doc/distilbert.html) called DistilBERT. It's been shown to be faster than BERT with similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the Hugging Face transformers library\n",
    "!pip install transformers -q\n",
    "\n",
    "import torch # our BERT model from Hugging Face uses PyTorch\n",
    "\n",
    "'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing\n",
    "\n",
    "BERT doesn't require much preprocessing. We just need to do the following:\n",
    "\n",
    "1. lowercase\n",
    "2. handle special characters (e.g. Ã¼)\n",
    "3. remove punctuation (we're considering each tweet as one sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode # remove accents from characters\n",
    "import html    # for html encoded characters (e.g. &amp;)\n",
    "import re      # regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    '''Normalize a text sample'''\n",
    "    \n",
    "    # unescape html\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # remove mentions\n",
    "    text = re.sub(r'(^|.)@[^\\s]*', r'', text)\n",
    "    \n",
    "    # remove urls\n",
    "    text = re.sub(r'https?:\\/\\/[^\\s]*', r'', text)\n",
    "    \n",
    "    # remove accented characters\n",
    "    text = unidecode(text)\n",
    "    \n",
    "    # remove unwanted characters\n",
    "    text = re.sub(r\"[^a-zA-Z\\s']+\", r' ', text)\n",
    "    \n",
    "    # remove repeated apostrophes\n",
    "    text = re.sub(r\"(['])[']+\", r'\\1', text)\n",
    "    \n",
    "    # remove whitespace from the sides\n",
    "    text = text.strip()\n",
    "    \n",
    "    # turn whitespace into a space\n",
    "    text = re.sub(r'\\s+', r' ', text)\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all our text\n",
    "df['text'] = '''TODO'''\n",
    "\n",
    "df['text'].iat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode\n",
    "\n",
    "1. Tokenize words into IDs in BERT's vocabulary\n",
    "2. Add `[CLS]` tokens to classify the text\n",
    "3. Add `[SEP]` tokens at the ends of sentences (BERT needs them surrounding each sentence)\n",
    "4. Pad tokens to the same length\n",
    "5. Create an attention mask to ignore padding\n",
    "\n",
    "If our samples were too long (> 512 tokens), we would have to truncate them or create a list of sentences. Luckily, tweets are quite short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# download a pretrained tokenizer (needs internet)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face has a BertNormalizer class that can do some of what our `clean()` function does\n",
    "\n",
    "```py\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "\n",
    "tokenizer.normalizer = BertNormalizer(\n",
    "    clean_text=True, handle_chinese_chars=True,\n",
    "    strip_accents=True, lowercase=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: list):\n",
    "    '''Encodes text\n",
    "    \n",
    "    Arguments:\n",
    "        text (list): Array of strings.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 3D array of encodings; (sample, [tokens, mask], value)\n",
    "    '''\n",
    "    \n",
    "    encodings = tokenizer('''TODO''')\n",
    "\n",
    "    # convert encodings into a 3D numpy array\n",
    "    encodings = np.stack(\n",
    "        [encodings.input_ids, encodings.attention_mask], axis=1)\n",
    "    \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all our text\n",
    "encodings = '''TODO'''\n",
    "encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are encoded IDs for words in BERT's vocabulary.\n",
    "\n",
    "Example from [Jay Alammar](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/#how-a-single-prediction-is-calculated)\n",
    "\n",
    "Input sentence  \n",
    "`a visually stunning rumination on love`\n",
    "\n",
    "Break words into tokens in BERT's vocabulary  \n",
    "`a` `visually` `stunning` `rum` `##ination` `on` `love`\n",
    "\n",
    "Add special tokens  \n",
    "`[CLS]` `a` `visually` `stunning` `rum` `##ination` `on` `love` `[SEP]`\n",
    "\n",
    "Encode tokens into IDs  \n",
    "`101` `1037` `17453` `14726` `19379` `12758` `2006` `2293` `102`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tokens\n",
    "encodings[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mask tells BERT which tokens are real (ones) and which are padded (zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mask\n",
    "encodings[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embed\n",
    "\n",
    "Use BERT to transform the text into embeddings (vectors of numbers to represent the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# download a pretrained model (needs internet)\n",
    "bert_model = AutoModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get class embeddings. Each chunk from a sample gets a vector of 768 embeddings (from the 768 hidden layers).\n",
    "\n",
    "Source: [Jay Alammar](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/#processing-with-distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_encodings(encodings):\n",
    "    '''Transfrom encoded samples.\n",
    "    \n",
    "    Arguments:\n",
    "        encodings (np.ndarray): 3D array of encodings;\n",
    "            (sample, [tokens, mask], value)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: 2D array of 768 hidden states for the '[CLS]' token\n",
    "            for each sample; (sample, embeddings)\n",
    "    '''\n",
    "\n",
    "    X = []\n",
    "\n",
    "    for sample in tqdm(encodings):\n",
    "        # we need our tokens and mask as a pytorch tensor\n",
    "        tokens = '''TODO'''\n",
    "        mask = '''TODO'''\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = bert_model('''TODO''')\n",
    "\n",
    "        # we only care about class embeddings\n",
    "        embeddings = last_hidden_states[0][:,0,:][0].numpy()\n",
    "        \n",
    "        X.append(embeddings)\n",
    "\n",
    "    # convert X into a 2D numpy array\n",
    "    X = np.stack(X, axis=0)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes a while, so we'll reduce the size of our training data. Once everything is working, switch to using a GPU transform all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced data size for convenience\n",
    "encodings = encodings[:500]\n",
    "\n",
    "# match input samples\n",
    "y = df[df.index < 500]\n",
    "y = y['target'].to_numpy()\n",
    "\n",
    "X = '''TODO: 2D numpy array of embeddings'''\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train and test data\n",
    "\n",
    "We need to split our training data into training and validation data. We use training data to train our model, then validation data to check our performance (and tweak our model if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = map(\n",
    "    lambda x: np.stack(x, axis=0),\n",
    "    train_test_split('''TODO''')\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN\n",
    "\n",
    "We'll use an artifical neural network on the sentence embeddings to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf    # deep learning library, like PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann(input_shape: tuple):\n",
    "    '''Builds an artifical neural network.'''\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        \n",
    "        tf.keras.layers.Dense('''TODO'''),\n",
    "        tf.keras.layers.Dropout('''TODO'''), # regularization\n",
    "        \n",
    "        tf.keras.layers.Dense('''TODO''')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our model\n",
    "ann_model = '''TODO'''\n",
    "\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the model as a flowchart\n",
    "tf.keras.utils.plot_model(ann_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# prevent overfitting\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ann_model.fit('''TODO''')\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.evaluate('''TODO''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Enable a GPU hardware accelerator at this point (this will take a while). You'll have to rerun the cells where you imported packages, declared functions, and created the BERT models we used.\n",
    "\n",
    "1. Clean training data set\n",
    "2. Use BERT to embed text\n",
    "3. Build ANN\n",
    "4. Train on all samples\n",
    "5. Predict labels on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib # check devices for TensorFlow\n",
    "\n",
    "# use cuda with PyTorch if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'using {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('no GPU avaiable')\n",
    "\n",
    "# TensorFlow automatically uses cuda\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(df):\n",
    "    '''Clean and Encode a data set.\n",
    "    \n",
    "    Arguments:\n",
    "        df (pandas.DataFrame): The data set, with samples in the 'text' column.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 3D array of encodings; (index, [tokens, mask], value)\n",
    "    '''\n",
    "    \n",
    "    # clean text\n",
    "    text = df['text'].apply(clean).to_list()\n",
    "\n",
    "    # encode text\n",
    "    encodings = encode_text(text)\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = '''TODO: training data'''\n",
    "\n",
    "y = '''TODO: numpy array of labels'''\n",
    "\n",
    "X = '''TODO: 2D numpy array of input features'''\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed text\n",
    "X = '''TODO'''\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ANN\n",
    "ann_model = '''TODO'''\n",
    "\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.fit('''TODO''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to predict disaster tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "df_test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "\n",
    "X_test = '''TODO'''\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = '''TODO'''\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model actually predicts probabilities (logits). We have to snap probabilites less than 50% to 0 and greater than 50% to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = ann_model.predict(X_test)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.apply_along_axis(lambda p: 1 if tf.greater(p[0], 0.5) else 0, 1, logits)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target'] = pred\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your predictions to a CSV file. We only want the `id` and `target` columns. Then go back to Kaggle and submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(labels=['text', 'keyword', 'location'], axis=1)\n",
    "df_test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
